# -*- coding: utf-8 -*-
"""Task6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u6JnLX1WriOw4JvVd1z2PozaLKldlV3O

**Importing libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from datetime import datetime

# Preprocessing
import os
import re
import nltk
nltk.download('wordnet')
nltk.download('stopwords')

from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer,PorterStemmer
from nltk.corpus import stopwords
import string
from sklearn import preprocessing

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from wordcloud import WordCloud

# Warnings 
import warnings
from scipy import stats
warnings.filterwarnings('ignore')

# Feature engineering
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

# Modeling
from sklearn import model_selection, naive_bayes, svm, metrics
from sklearn.metrics import f1_score
from sklearn.svm import SVC

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_confusion_matrix

from google.colab import drive
drive.mount('/content/drive')

train = pd.read_csv('drive/Shared drives/LHS712-Task6/Dataset/train.tsv', sep='\t')
valid = pd.read_csv('drive/Shared drives/LHS712-Task6/Dataset/valid.tsv', sep='\t')
all_data = pd.read_csv('drive/Shared drives/LHS712-Task6/Dataset/train_valid.tsv', sep='\t',
                       header = 0, names = ["tweetID","tweet", "label"])

"""**Text Pre-processing**"""

lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()


def clean_tweets(tweet):
    tweet=str(tweet)
    tweet = tweet.lower()
    tweet=tweet.replace("{html}","") 
    remove_tweet = re.compile("<.*?>")
    remove_text = re.sub(remove_tweet, "", tweet)
    remove_url=re.sub(r"http\S+", "", remove_text)
    remove_num = re.sub("[0-9]+", "", remove_url)
    #since the word 'amp' is overlapping and we don't need it, we removed it.
    remove_words = re.sub(r'\bamp\b', '', remove_num)
    
    token = RegexpTokenizer(r"\w+")
    word_token = token.tokenize(remove_words)  
    filtered_tweet = [i for i in word_token if len(i) > 2 if not i in stopwords.words('english')]
    stemmer_tweet=[stemmer.stem(i) for i in filtered_tweet]
    lemmatizer_tweet=[lemmatizer.lemmatize(i) for i in stemmer_tweet]
    return " ".join(filtered_tweet)

train['tweet'] = train['tweet'].map(lambda s:clean_tweets(s)) 
valid['tweet'] = valid['tweet'].map(lambda s:clean_tweets(s))

"""**Word Cloud Visualization**"""

comment_words = ''  # concatenation of all tweets
for tws in train.tweet:
      
    # typecaste each val to string
    tws = str(tws)
  
    # split the value
    tokens = tws.split()
     
    comment_words += " ".join(tokens)+" "
  
wordcloud = WordCloud(width = 1200, height = 800,
                background_color ='white',
                min_font_size = 10).generate(comment_words)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
  
plt.show()

"""**Creating training and test datasets**"""

train_x = train['tweet']
train_y = train['label']
test_x = valid['tweet']
test_y = valid['label']

# Encode target labels to numeric
# Lit-News_mentions = 0
# Nonpersonal_reports = 1
# Self_reports = 2
Encoder = LabelEncoder()
train_y = Encoder.fit_transform(train_y)
test_y = Encoder.fit_transform(test_y)

pd.crosstab(index=train_y,     # Make a crosstab
                      columns="prop", normalize='all')      # Name the count column

pd.crosstab(index=test_y,     # Make a crosstab
                      columns="prop", normalize='all')      # Name the count column

pd.crosstab(index=all_data['label'],     # Make a crosstab
                      columns="prop", normalize='all')      # Name the count column

"""**Feature Engineering: TFIDF**"""

Tfidf_vect = TfidfVectorizer(max_features=1000)
Tfidf_vect.fit(train_x)
train_x_tfidf = Tfidf_vect.transform(train_x)
test_x_tfidf = Tfidf_vect.transform(test_x)

"""**Feature Engineering: Count Vectorizer**"""

count_vect = CountVectorizer()
count_vect.fit(train_x)
train_x_count = count_vect.transform(train_x)
test_x_count = count_vect.transform(test_x)

"""**Moving forward, you will want to use the following datasets for model fitting and prediction.**

Train:

* train_x_tfidf: training TFIDF features

* train_x_count: training count vect. features

* train_y: training labels

Test: 

* test_x_tfidf: test TFIDF featurers

* test_x_count: test count vect. features

* test_y: test labels

**Naive Bayes**
"""

# NB Classifier w/ Tfidf
nb_classifier_tfidf = naive_bayes.MultinomialNB()
nb_classifier_tfidf.fit(train_x_tfidf, train_y)
y_pred_nb_tfidf = nb_classifier_tfidf.predict(test_x_tfidf)
nb_tfidf_acc = metrics.accuracy_score(test_y, y_pred_nb_tfidf)
nb_tfidf_f1 = metrics.f1_score(test_y, y_pred_nb_tfidf, average = 'weighted')

print(metrics.classification_report(test_y, y_pred_nb_tfidf,
                                            target_names=['Lit-News_mentions', 'Nonpersonal_reports',
                                                          'Self_reports']))
print('------------------------------')
print("confusion matrix:")
print(metrics.confusion_matrix(test_y, y_pred_nb_tfidf))
print('------------------------------')
print("Number of mislabeled points out of a total %d points : %d" % (test_x_tfidf.shape[0], (test_y != y_pred_nb_tfidf).sum()))

# NB Classifier w/ Count Vect
nb_classifier_ct = naive_bayes.MultinomialNB()
nb_classifier_ct.fit(train_x_count, train_y)
y_pred_nb_ct = nb_classifier_ct.predict(test_x_count)
nb_ct_acc = metrics.accuracy_score(test_y, y_pred_nb_ct)
nb_ct_f1 = metrics.f1_score(test_y, y_pred_nb_ct, average = 'weighted')

print(metrics.classification_report(test_y, y_pred_nb_ct,
                                            target_names=['Lit-News_mentions', 'Nonpersonal_reports',
                                                          'Self_reports']))
print('------------------------------')
print("confusion matrix:")
print(metrics.confusion_matrix(test_y, y_pred_nb_ct))
print('------------------------------')
print("Number of mislabeled points out of a total %d points : %d" % (test_x_count.shape[0], (test_y != y_pred_nb_ct).sum()))

"""**Random Forest**"""

# Random Forest with tfidf
rf_classifier_tfidf = RandomForestClassifier()
rf_classifier_tfidf.fit(train_x_tfidf, train_y)
y_pred_rf_tfidf = rf_classifier_tfidf.predict(test_x_tfidf)

rf_tfidf_acc = metrics.accuracy_score(test_y, y_pred_rf_tfidf)
rf_tfidf_f1 = metrics.f1_score(test_y, y_pred_rf_tfidf, average = 'weighted')

print(metrics.classification_report(test_y, y_pred_rf_tfidf,
                                            target_names=['Lit-News_mentions', 'Nonpersonal_reports',
                                                          'Self_reports']))
print('------------------------------')
print("confusion matrix:")
print(metrics.confusion_matrix(test_y, y_pred_rf_tfidf))
print('------------------------------')
print("Number of mislabeled points out of a total %d points : %d" % (test_x_tfidf.shape[0], (test_y != y_pred_rf_tfidf).sum()))

# Random Forest with Count Vectorizor 

rf_classifier_count = RandomForestClassifier()
rf_classifier_count.fit(train_x_count, train_y)
y_pred_rf_count = rf_classifier_count.predict(test_x_count)

rf_count_acc = metrics.accuracy_score(test_y, y_pred_rf_count)
rf_count_f1 = metrics.f1_score(test_y, y_pred_rf_count, average = 'weighted')

print(metrics.classification_report(test_y, y_pred_rf_count,
                                            target_names=['Lit-News_mentions', 'Nonpersonal_reports',
                                                          'Self_reports']))
print('------------------------------')
print("confusion matrix:")
print(metrics.confusion_matrix(test_y, y_pred_rf_tfidf))
print('------------------------------')
print("Number of mislabeled points out of a total %d points : %d" % (test_x_count.shape[0], (test_y != y_pred_rf_count).sum()))

"""**SVM**"""

# Hyperparameter tuning for SVM
# defining parameter range 
param_grid = {'C': [0.1, 1, 10, 50, 100],  
              'gamma': ['scale'], 
              'kernel': ['rbf', 'sigmoid', 'poly']}  
  
grid = model_selection.GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3) 
  
# fitting the model for grid search 
grid.fit(train_x_tfidf, train_y)

# print best parameter after tuning
print(grid.best_params_) 

# print how our model looks after hyper-parameter tuning 
print(grid.best_estimator_)

# svm with tfidf
svm_classifier_tfidf = SVC(C=50, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
svm_classifier_tfidf.fit(train_x_tfidf, train_y)
y_pred_svm_tfidf = svm_classifier_tfidf.predict(test_x_tfidf)

svm_tfidf_acc = metrics.accuracy_score(test_y, y_pred_svm_tfidf)
svm_tfidf_f1 = metrics.f1_score(test_y, y_pred_svm_tfidf, average = 'weighted')

print(metrics.classification_report(test_y, y_pred_svm_tfidf,
                                            target_names=['Lit-News_mentions', 'Nonpersonal_reports',
                                                          'Self_reports']))
print('------------------------------')
print("confusion matrix:")
print(metrics.confusion_matrix(test_y, y_pred_svm_tfidf))
print('------------------------------')
print("Number of mislabeled points out of a total %d points : %d" % (test_x_tfidf.shape[0], (test_y != y_pred_svm_tfidf).sum()))

# svm Classifier w/ Count Vect
svm_classifier_ct = SVC(C=50, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
svm_classifier_ct.fit(train_x_count, train_y)
y_pred_svm_ct = svm_classifier_ct.predict(test_x_count)
svm_ct_acc = metrics.accuracy_score(test_y, y_pred_svm_ct)
svm_ct_f1 = metrics.f1_score(test_y, y_pred_svm_ct, average = 'weighted')

print(metrics.classification_report(test_y, y_pred_svm_ct,
                                            target_names=['Lit-News_mentions', 'Nonpersonal_reports',
                                                          'Self_reports']))
print('------------------------------')
print("confusion matrix:")
print(metrics.confusion_matrix(test_y, y_pred_svm_ct))
print('------------------------------')
print("Number of mislabeled points out of a total %d points : %d" % (test_x_count.shape[0], (test_y != y_pred_svm_ct).sum()))

# SVM with Tfidf 
svm_clf = SVC(C=50, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
SVM = svm_clf.fit(train_x_tfidf, train_y)
y_pred_svm = SVM.predict(test_x_tfidf)
svm_f1_tfidf = f1_score(test_y, y_pred_svm, average = 'weighted')
print("f1 score: {}".format(svm_f1_tfidf))
print("Number of mislabeled points out of a total %d points : %d" % (test_x_tfidf.shape[0], (test_y != y_pred_svm).sum()))

# Hyperparameter tuning for SVM
# defining parameter range 
param_grid = {'C': [0.1, 1, 10, 50, 100],  
              'gamma': ['scale'], 
              'kernel': ['rbf', 'sigmoid', 'poly']}  
  
grid = model_selection.GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3) 
  
# fitting the model for grid search 
grid.fit(train_x_count, train_y)

# print best parameter after tuning
print(grid.best_params_) 

# print how our model looks after hyper-parameter tuning 
print(grid.best_estimator_)

# SVM with Count Vectorizor 
svm_clf_count = SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='scale', kernel='sigmoid',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
SVM_count = svm_clf_count.fit(train_x_count, train_y)
y_pred_svm_count= SVM_count.predict(test_x_count)
svm_f1_count = f1_score(test_y, y_pred_svm_count, average = 'weighted')
print("f1 score: {}".format(svm_f1_count))
print("Number of mislabeled points out of a total %d points : %d" % (test_x_count.shape[0], (test_y != y_pred_svm_count).sum()))

"""**Confusion Matrix for SVM w/ Count Vectorization**"""

titles_options = [("Confusion matrix, without normalization", None),
                  ("Normalized confusion matrix", 'true')]
for title, normalize in titles_options:
    fig, ax = plt.subplots(figsize=(7, 7))
    disp = plot_confusion_matrix(SVM_count, test_x_count, test_y,
                                 display_labels=['Lit/News mentions', 'Nonpersonal reports',
                                                          'Self reports'],
                                 cmap=plt.cm.Blues,
                                 normalize=normalize,
                                 ax = ax)
    
    disp.ax_.set_title(title)
 
    print(disp.confusion_matrix)


plt.show()

"""**F-1 Score**"""

print('-----------COUNT_f1_SCORE----------')
print("f1_NB_score_count: {}".format(nb_ct_f1))
print("f1_RF_score_count: {}".format(rf_count_f1))
print("f1_SVM_score_count: {}".format(svm_f1_count))
print('\n-----------TFIDF_f1_SCORE----------')
print("f1_NB_score_tfidf: {}".format(nb_tfidf_f1))
print("f1_RF_score_tfidf: {}".format(rf_tfidf_f1))
print("f1_SVM_score_tfidf: {}".format(svm_f1_tfidf))

# list of f1_scores
dict_clf = {'nb_ct_f1': nb_ct_f1, 'rf_count_f1': rf_count_f1, 'svm_f1_count': svm_f1_count 
           ,'nb_tfidf_f1': nb_tfidf_f1, 'rf_tfidf_f1': rf_tfidf_f1, 'svm_f1_tfidf':svm_f1_tfidf}
  
# printing the maximum element
print("\nThe highest f1 score is:", max(dict_clf.items(), key=lambda k: k[1]))

#f_1 score graph

dict_count ={'Naive Bayes': nb_ct_f1, 'Random Forest': rf_count_f1, 'SVM': svm_f1_count}
dict_tfidf ={'Naive Bayes': nb_tfidf_f1, 'Random Forest': rf_tfidf_f1, 'SVM':svm_f1_tfidf}

# line 1 points
x1 = ['Naive Bayes','Random Forest','SVM']
y1 = [nb_ct_f1,rf_count_f1,svm_f1_count]
# plotting the line 1 points 
plt.plot(x1, y1, label = "Counter Vectorizer")
# line 2 points
x2 = ['Naive Bayes','Random Forest','SVM']
y2 = [nb_tfidf_f1,rf_tfidf_f1,svm_f1_tfidf]
# plotting the line 2 points 
plt.plot(x2, y2, label = "TFIDF Vectorizer")
plt.xlabel('Models')
# Set the y axis label of the current axis.
plt.ylabel('F_1 Score')
# Set a title of the current axes.
plt.title('F-1 score for each classification')
# show a legend on the plot
plt.legend()
# Display a figure.
plt.show()

svm_decoded_labels = Encoder.inverse_transform(y_pred_svm_count)
valid['SVM_Label(count)'] = svm_decoded_labels
valid.to_csv("drive/Shared drives/LHS712-Task6/Dataset/svm_test_valid", sep = ',', index = False)

# Unmatched test cases
unmatched_svm = valid[valid['label'] != valid['SVM_Label(count)']]
unmatched_svm.info()
unmatched_svm.to_csv("drive/Shared drives/LHS712-Task6/Dataset/svm_test_unmatched.csv", sep = ',', index = False)